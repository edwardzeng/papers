# GPT-Academic Report
# Title:



Retrieval-Augmented Generation for Large Language Models: A Survey

# Abstract:



Large language models (LLMs) demonstrate powerful capabilities, but they still face challenges in practical applications, such as hallucinations, slow knowledge updates, and lack of transparency in answers. Retrieval-Augmented Generation (RAG) refers to the retrieval of relevant information from external knowledge bases before answering questions with LLMs. RAG has been demonstrated to significantly enhance answer accuracy, reduce model hallucination, particularly for knowledge-intensive tasks. By citing sources, users can verify the accuracy of answers and increase trust in model outputs. It also facilitates knowledge updates and the introduction of domain-specific knowledge. RAG effectively combines the parameterized knowledge of LLMs with non-parameterized external knowledge bases, making it one of the most important methods for implementing large language models. This paper outlines the development paradigms of RAG in the era of LLMs, summarizing three paradigms: Naive RAG, Advanced RAG, and Modular RAG. It then provides a summary and organization of the three main components of RAG: retriever, generator, and augmentation methods, along with key technologies in each component. Furthermore, it discusses how to evaluate the effectiveness of RAG models, introducing two evaluation methods for RAG, emphasizing key metrics and abilities for evaluation, and presenting the latest automatic evaluation framework. Finally, potential future research directions are introduced from three aspects: vertical optimization, horizontal scalability, and the technical stack and ecosystem of RAG.1

Footnote 1: Resources are available at: https://github.com/Tongji-KGLLM/RAG-Survey



# Meta Translation

标题: 检索增强生成的大型语言模型：一项综述

作者: 高云帆 \({}^{1}\)

熊云 \({}^{2}\)

高新宇 \({}^{2}\)

贾康祥 \({}^{2}\)

刘金流 

摘要: 大型语言模型(LLMs)展示了强大的能力，但在实际应用中仍面临挑战，如虚构、知识更新缓慢以及答案的透明度不足。检索增强生成(RAG)指的是在回答问题之前从外部知识库检索相关信息。RAG已被证明可以显著提高答案准确性，减少模型虚构，特别适用于知识密集型任务。通过引用来源，用户可以验证答案的准确性并增加对模型输出的信任。它还促进了知识更新和引入领域专门的知识。RAG有效地将LLMs的参数化知识与非参数化的外部知识库结合起来，使其成为实现大型语言模型的最重要的方法之一。本文概述了在LLMs时代的RAG发展范式，总结了三种范式：Naive RAG，Advanced RAG和Modular RAG。然后，它提供了RAG的三个主要组成部分：检索器(retriever)，生成器(generator)和增强方法(augmentation methods)的摘要和组织，并介绍了每个组成部分中的关键技术。此外，它还讨论了如何评估RAG模型的有效性，介绍了两种用于RAG的评估方法，强调了关键指标和能力，并呈现了最新的自动评估框架。最后，从垂直优化、水平可扩展性以及RAG的技术堆栈和生态系统的三个方面介绍了潜在的未来研究方向。1

脚注1: 资源可在以下链接找到: https://github.com/Tongji-KGLLM/RAG-Survey

# 1 Introduction
# 引言

大型语言模型（LLM）比我们在自然语言处理（NLP）领域见过的任何模型都更强大。GPT系列模型[17, 18]、LLama系列模型[16]、Gemini[19]和其他大型语言模型在语言和知识掌握方面取得了令人瞩目的成就，在多个评估基准[20, 14, 15]上超过了人类水平。

然而，大型语言模型也存在许多不足之处。它们常常编造事实[13]，在处理特定领域或高度专业化查询时缺乏知识[13]。例如，当所需的信息超出模型的训练数据范围或需要最新数据时，LLM可能无法提供准确的答案。这种限制在实际的生产环境中部署生成式人工智能时会带来挑战，因为仅仅使用黑箱LLM可能不足够。

传统上，神经网络通过微调模型以实现对特定领域或专有信息的适应。虽然这种技术取得了显著的结果，但它需要大量的计算资源、高成本，并需要专门的技术专长，使其难以适应不断变化的信息景观。参数化知识和非参数化知识发挥着不同的作用。参数化知识是通过训练LLM并存储在神经网络权重中获得的，代表了模型对训练数据的理解和概括，构成了生成响应的基础。另一方面，非参数化知识存在于外部知识源（如向量数据库）中，没有直接编码到模型中，而是作为可更新的补充信息进行处理。非参数化知识使LLM能够访问和利用最新的或领域特定的信息，提高了响应的准确性和相关性。

纯粹参数化的语言模型（LLM）将其从大量语料中获取的世界知识存储在模型的参数中。然而，这种模型也有其局限性。首先，难以保留训练语料中的所有知识，尤其是不常见和更具体的知识。其次，由于模型参数无法动态更新，参数化知识随时间的推移可能变得过时。最后，参数的增加导致了训练和推断的计算开销增加。为了解决纯粹参数化模型的局限性，语言模型可以采用一种半参数化的方法，将非参数化的语料库数据库与参数化模型相结合。这种方法被称为检索增强生成（RAG）。

检索增强生成（RAG）这一术语最早由[16]提出。它将预训练的检索器与预训练的seq2seq模型（生成器）结合，并进行端到端微调，以更可解释和模块化的方式捕捉知识。在大模型出现之前，RAG主要关注端到端模型的直接优化。在检索端进行密集的检索，例如使用基于向量的密集通道检索（DPR）[15]，然后在生成端训练较小的模型是常见做法。由于参数规模总体较小，检索器和生成器通常会进行同步的端到端训练或微调[14]。

在像ChatGPT这样的大型语言模型（LLM）出现之后，生成式语言模型成为主流，展示了在各种语言任务上令人印象深刻的性能。然而，LLMs仍然面临幻觉、知识更新和数据相关问题等挑战。这影响了LLMs的可靠性，在某些严峻的任务场景中表现不佳，特别是在需要访问大量知识的知识密集型任务中，如开放域问答和常识推理。参数中的隐含知识可能是不完整和不充分的。

随后的研究发现，将RAG引入到大型模型的In-Context Learning（ICL）中可以缓解上述问题，产生显著且易于实现的效果。在推理过程中，RAG动态地从外部知识源中检索信息，使用检索到的数据作为参考来组织答案。这大大提高了回应的准确性和相关性，有效解决了LLMs中存在的幻觉问题。自LLM出现以来，这项技术迅速得到了关注，已经成为改进聊天机器人和使LLM更加实用的热门技术之一。通过将事实知识与LLMs的训练参数分离，RAG巧妙地将生成模型的强大能力与检索模块的灵活性相结合，为纯参数化模型固有的不完整和不充分的知识问题提供了有效的解决方案。

本文系统地回顾和分析了RAG的当前研究方法和未来发展路径，将其总结为三个主要范式：Naive RAG、Advanced RAG和Modular RAG。随后，本文对检索、增强和生成这三个核心组成部分进行了综合总结，突出了RAG的改进方向和当前的技术特点。在增强方法部分，本文将当前工作分为三个方面：RAG的增强阶段、增强数据源和增强过程。此外，本文还总结了与RAG相关的评估系统、适用场景和其他相关内容。通过本文，读者可以更全面、系统地了解大型模型和检索增强生成。他们将熟悉知识检索增强的演化路径和关键技术，能够区分不同技术的优缺点，确定适用场景，并探索实践中的当前典型应用案例。值得注意的是，在以前的工作中，Feng等[21]系统地回顾了将大型模型与知识相结合的方法、应用和未来趋势，重点关注知识编辑和检索增强方法。Zhu等[21]介绍了增强大型语言模型检索系统的最新进展。与之对比，本文旨在系统概述检索增强生成（RAG）的整个过程，并专注于通过知识检索增强大型语言模型生成相关的研究。

图1展示了RAG算法和模型的发展情况。在时间轴上，与RAG相关的大部分研究在2020年后出现，尤其是2022年12月ChatGPT发布之后，研究进入了大模型时代。Naive RAG技术迅速崭露头角，相关研究数量迅速增加。在增强策略方面，自RAG的概念提出以来，关于预训练和监督微调阶段的增强研究一直在进行中。然而，在推理阶段进行强化学习的研究大多出现在大模型时代，主要是因为高性能大模型的训练成本较高。研究人员尝试通过在推理阶段引入RAG模块，以经济高效的方式将外部知识融入到模型生成中。在增强数据的使用方面，早期的RAG主要集中在应用非结构化数据，特别是在开放领域的问答上。随后，为了解决大模型中的错误内化和幻觉等问题，知识检索的范围扩展了，运用高质量数据作为知识源。其中包括结构化知识，以知识图谱为代表。最近，自检索引起了更多注意，它包括挖掘LLMs自身的知识来提升性能。

本文的后续章节结构如下：第2章介绍了RAG的背景。第3章介绍了RAG的主流范式。第4章分析了RAG中的检索模块。第5章重点介绍了RAG中的生成模块。第6章着重介绍了RAG中的增强方法。第7章介绍了RAG的评估体系。第8章展望了RAG的未来发展趋势。最后，第9章对调查报告的主要内容进行总结。

# 2 Background
# 背景

在本章中，我们将介绍RAG的定义，以及RAG与其他模型优化技术（如微调）的比较。

Retrieval-Augmented Generation（RAG）是指在使用大型语言模型（LLM）回答问题之前，从外部知识库中检索相关信息的过程。虽然LLM展现出强大的能力，但在实际应用中仍面临一些挑战，例如产生幻觉、知识更新缓慢以及答案的不透明性。RAG通过引用来源，用户可以验证答案的准确性，增加对模型输出的信任。它还促进了知识更新和引入特定领域知识的过程。RAG有效地将LLM的参数化知识与非参数化的外部知识库相结合，使其成为实现大型语言模型的最重要的方法之一。

本文概述了在LLM时代中RAG的发展范式，总结了三种范式：Naive RAG、Advanced RAG和Modular RAG。然后，对RAG的三个主要组成部分进行了总结和组织：检索器、生成器和增强方法，以及每个部分中的关键技术。

此外，本文还讨论了如何评估RAG模型的有效性，介绍了两种RAG的评估方法，强调了评估的关键指标和能力，并提供了最新的自动评估框架。

最后，从垂直优化、水平扩展性以及RAG的技术堆栈和生态系统的三个方面，介绍了潜在的未来研究方向。

备注1：资源可在以下链接找到：https://github.com/Tongji-KGLLM/RAG-Survey

# Definition
随着技术的发展，RAG的含义也得到了扩展。在大型语言模型时代，RAG的具体定义是指模型在回答问题或生成文本时，首先从海量的文档语料库中检索相关信息。随后，利用这些检索到的信息来生成回答或文本，从而提高预测的质量。RAG方法使开发者能够避免每个特定任务都需要重新训练整个大型模型的需求。相反，他们可以连接一个知识库，为模型提供额外的信息输入，改进其回答的准确性。RAG方法特别适用于知识密集型任务。总结起来，RAG系统包括两个关键阶段：

1. 利用编码模型根据问题检索相关文档，例如BM25、DPR、ColBERT等方法[16, 17, 18]。
2. 生成阶段：利用检索到的上下文作为条件，系统生成文本。

# RAG vs Fine-tuning
在对大型语言模型（LLM）进行优化时，除了RAG外，另一个重要的优化技术是微调。

RAG类似于向模型提供教科书，使其能够根据特定查询检索信息。这种方法适用于模型需要回答特定问题或处理特定信息检索任务的场景。然而，RAG不适用于教导模型理解广泛领域或学习新语言、格式或风格的情况。

微调类似于让学生通过广泛学习内化知识。这种方法在模型需要复制特定结构、风格或格式时非常有用。微调可以提高非微调模型的性能，使交互更高效。它特别适合强调基础模型中的现有知识，修改或自定义模型的输出，并为模型提供复杂的指令。然而，微调不适合将新知识纳入模型或应对需要快速迭代的新用例的情况。

图1：现有RAG研究的时间线。时间线主要是根据发布日期确定的。

微调类似于让学生通过长时间学习内化知识。该方法适用于模型需要复制特定的结构、风格或格式的情况。微调可以实现性能优于非微调模型，并提高交互效率。微调尤其适合强调基础模型中的现有知识，修改或自定义模型的输出，并用复杂指令指导模型。然而，微调不适用于向模型添加新知识或需要对新用例进行快速迭代的情况。RAG和Fine-tuning（FT）之间的具体比较可以在表1中详细说明。

RAG和微调并不是互斥的，可以相互补充，提高模型在不同层面上的能力。在某些情况下，将这两种技术结合起来可以实现最佳模型性能。使用RAG和微调进行优化的整个过程可能需要多次迭代才能达到令人满意的结果。

现有研究显示，与其他优化大型语言模型的方法相比，检索增强生成（RAG）具有显著优势[22, 23, 24, 25]：

* RAG通过与外部知识相关联来提高准确性，减少语言模型中的错误生成问题，使生成的回答更准确可靠。
* 使用检索技术可以找到最新的信息。与仅依赖训练数据的传统语言模型相比，RAG可以保持响应的及时性和准确性。
* RAG具有透明性的优势。通过引用来源，用户可以验证回答的准确性，增加对模型输出的信任。
* RAG具有定制能力。通过索引相关文本语料库，可以为不同领域定制模型，提供特定领域的知识支持。
* 在安全和隐私管理方面，RAG在数据库中具有内置的角色和安全控制，可以更好地控制数据的使用。相比之下，微调模型可能缺乏清晰的管理，无法确定谁可以访问哪些数据。
* RAG具有更好的可扩展性。它可以处理大规模数据集，而无需更新所有参数和创建训练集，从而更具经济高效性。
* 最后，RAG产生的结果更可信赖。RAG从最新数据中选择确定性结果，而微调模型在处理动态数据时可能出现错误生成和不准确性的问题，缺乏透明度和可信度。

# 3 RAG Framework
第三章：RAG框架

RAG研究范式不断发展。本章主要介绍了RAG研究范式的演变过程。我们将其分为三种类型：Naive RAG，Advanced RAG和Modular RAG。尽管早期的RAG在成本效益和性能上表现优于原始LLM，但仍然面临许多不足之处。Advanced RAG和Modular RAG的出现旨在解决Naive RAG中的特定缺陷。

图2：RAG与其他模型优化方法的比较

# Naive RAG
Naive RAG研究范式是在ChatGPT广泛应用后迅速崭露头角的最早方法论。Naive RAG采用传统的索引、检索和生成过程。Naive RAG也被总结为一个"检索"-"读取"框架[13]。

# 3.1.1 Indexing
3.1.1 索引构建

从源头获取数据并为其构建索引的流程通常在离线状态下进行。具体而言，数据索引的构建包括以下步骤：

1. 数据索引: 这包括对原始数据进行清理和提取，将不同的文件格式（如PDF、HTML、Word、Markdown等）转换为纯文本。

2. 分块: 这涉及将加载的文本划分为较小的块。这是必要的，因为语言模型通常对其处理的上下文量有限，所以需要尽可能创建较小的文本块。

3. 嵌入和索引的创建：这是通过语言模型将文本编码成向量的过程。生成的向量将在后续的检索过程中用于计算向量与问题向量之间的相似度。嵌入模型需要具备较高的推断速度，因为需要对大量的语料进行编码，并在用户提问时实时编码问题。

表1显示了RAG与参数微调之间的特征比较：
```
\begin{table}
\begin{tabular}{c c c} \hline \hline 特征比较 & RAG & 参数微调 \\ \hline \multirow{3}{*}{知识更新} & 直接更新检索知识库，保证信息的及时性，无需频繁重新训练，适用于动态数据环境。 & \multirow{3}{*}{\begin{tabular}{c}存储静态数据，需要重新训练以更新知识和数据。\\ \end{tabular} } \\  & & \\ \hline \multirow{3}{*}{外部知识} & 擅长使用外部资源，特别适用于文档或其他结构化/非结构化数据库。 & 可以用于将预训练的外部学习到的知识与大型语言模型进行对齐，但对于频繁变化的数据源可能不太实用。 \\ \cline{2-3}  & \begin{tabular}{c} 需要最少的数据处理和处理。 \\ \end{tabular} & \begin{tabular}{c} 需要构建高质量的数据集，并且有限的数据集可能不会带来显著的性能提升。\\ \end{tabular} \\ \hline \multirow{3}{*}{模型定制} & 侧重于信息检索和整合外部知识，但可能无法完全定制模型行为或写作风格。 & 可根据特定的口气或术语调整LLM行为、写作风格或特定领域知识。 \\ \hline \multirow{3}{*}{可解释性} & 答案可以追溯到具体的数据源，提供更高的可解释性和追踪性。 & 类似于黑盒，不总是清楚模型为什么以某种方式反应，可解释性相对较低。 \\ \cline{2-3}  & \begin{tabular}{c} 需要计算资源来支持与数据库相关的检索策略和技术。需要维护外部数据源的整合和更新。 \\ \end{tabular} & \begin{tabular}{c} 需要准备和策划高质量的训练数据集，定义微调目标，并提供相应的计算资源。\\ \end{tabular} \\ \hline 延迟要求 & \begin{tabular}{c} 涉及数据检索，可能导致较高的延迟。 \\ \end{tabular} & \begin{tabular}{c} 微调后的LLM可以在没有检索的情况下响应，导致较低的延迟。 \\ \end{tabular} \\ \hline 减少幻觉 & \begin{tabular}{c} 由于每个答案都基于检索到的证据，因此内在上不容易出现幻觉。 \\ \end{tabular} & \begin{tabular}{c} 通过根据特定领域的数据对模型进行训练，可以帮助减少幻觉，但在面对不熟悉的输入时仍可能出现幻觉。 \\ \end{tabular}\\ \hline 道德和隐私问题 & \begin{tabular}{c} 存储和从外部数据库检索文本可能引发道德和隐私问题。\\ \end{tabular} & 
\begin{tabular}{c} 由于训练数据中可能存在敏感内容，可能会引发道德和隐私问题。 \\ \end{tabular} \\ \hline \hline \end{tabular}
\end{table}
```
表1：RAG与参数微调的比较。模型的参数规模不应过大。在生成嵌入向量后，下一步是创建索引，将原始语料的块和嵌入，以键-值对的形式存储，以便快速且频繁地进行搜索。

# Retrieve
对于用户的输入，使用与第一阶段相同的编码模型将查询转化为向量。计算问题嵌入与语料库中文档块的嵌入之间的相似度。根据相似度水平，选择前K个文档块作为当前问题的增强上下文信息。

# Generation
给定的问题和相关文档被合并为一个新的提示。然后，大型语言模型被要求根据提供的信息回答问题。根据不同任务的需求，可以决定是否允许大模型使用其知识，或仅根据给定的信息回答。如果存在历史对话信息，还可以将其合并到提示中以进行多轮对话。

# Drawbacks in Naive RAG
Naive RAG中存在三个主要挑战：检索质量、回答生成质量以及增强过程。

在检索质量方面，存在多个问题。主要问题是低精度，即在检索集合中并非所有的块都与查询相关，从而可能导致虚构和中间断层问题。其次是低召回率，即未能检索到所有相关块，从而无法为LLM提供足够的上下文来合成答案。此外，过时的信息也是一个挑战，数据的冗余或过时可能导致不准确的检索结果。

在回答生成质量方面，问题同样复杂多样。虚构是一个显著问题，即模型虚构了不存在于上下文中的答案。不相关性是另一个关注点，即模型生成了未能回答查询的答案。此外，毒性或偏见也是一个问题，即模型生成了有害或冒犯性的回应。

最后，增强过程也面临几个挑战。关键问题是如何有效地将检索到的段落上下文与当前生成任务相结合。如果处理不当，输出可能会显得不连贯或不协调。冗余和重复也是一个问题，特别是当多个检索到的段落包含相似信息时，在生成步骤中可能导致内容重复。此外，确定多个检索到的段落对于生成任务的重要性或相关性是具有挑战性的，增强过程需要恰当地平衡每个段落的价值。检索到的内容还可能来自不同的写作风格或语调，增强过程需要解决这些差异以确保输出的一致性。最后，生成模型可能过度依赖增强信息，导致输出仅仅重复了检索到的内容，而没有提供新的价值或综合性信息。

# Advanced RAG
进阶的 RAG 对于弥补 Naive RAG 的不足做出了有针对性的改进。在检索生成质量方面，进阶的 RAG 引入了预检索和后检索方法。为了解决 Naive RAG 遇到的索引问题，进阶的 RAG 通过滑动窗口、细粒度分段和元数据等方法对索引进行了优化。同时，它提出了各种方法来优化检索过程。在具体实现方面，进阶的 RAG 可以通过管道或端到端的方式进行调整。

# Pre-Retrieval Process
# Pre-Retrieval Process

### 1. 优化数据索引

优化数据索引的目的是提高索引内容的质量。目前，有五种主要的策略用于达到这个目的：增加索引数据的粒度、优化索引结构、添加元数据、对齐优化和混合检索。

1. **增加数据粒度：** 预索引优化的目标是改善文本标准化、一致性，并确保事实准确性和上下文丰富性，以确保 RAG 系统性能的稳定。文本标准化包括去除不相关的信息和特殊字符，以增强检索器的效率。在一致性方面，主要任务是消除实体和术语的歧义，同时消除重复或多余的信息以简化检索器的关注点。确保事实准确性非常重要，每个数据片段的准确性应尽可能得到验证。通过添加领域特定的注释，并通过用户反馈循环不断更新，可以实现上下文保留，以适应系统在真实世界的交互环境中的上下文。时间敏感性是重要的上下文信息，应设计机制来刷新过期的文档。总之，优化索引数据的重点应该是清晰、上下文和正确性，以使系统高效可靠。以下是最佳实践。

2. **优化索引结构：** 这可以通过调整块的大小、改变索引路径和引入图结构信息来实现。调整块的方法（由小到大）旨在收集尽可能多的相关上下文并减少噪音。在构建 RAG 系统时，块大小是一个关键参数。有不同的评估框架比较单个块的大小。例如，LlamaIndex2 使用 GPT4 评估准确性和相关性，LLaMA [21] 指标有用于不同块切割方法的自动评估功能。跨多个索引路径查询的方法与之前的元数据过滤和块切割方法密切相关，并且可能涉及同时在不同索引之间查询。可以使用标准索引来查询特定的查询，或者使用独立的索引来基于元数据关键字（例如特定的“日期”索引）进行搜索或过滤。引入图结构涉及将实体转化为节点，将实体之间的关系转化为关系。这可以通过利用节点之间的关联关系来提高准确性，尤其适用于多跳问题。使用图数据索引可以增加检索的相关性。

3. **添加元数据信息：** 这里的重点是将引用的元数据嵌入到块中，例如日期和用于过滤的目的。添加类似章节和引用的 subsections 的元数据也有助于改进检索。当我们将索引划分为多个块时，检索效率成为一个问题。通过首先通过元数据进行过滤可以提高效率和相关性。

4. **对齐优化：** 此策略主要解决文档之间的对齐问题和差异。对齐概念包括引入“假设性问题”，创建适合使用每个文档回答的问题，并将这些问题嵌入（或替换）到文档中。这有助于解决文档之间的对齐问题和差异。

5. **混合检索：** 此策略的优势在于利用不同检索技术的优点。智能地结合各种技术，包括基于关键词的搜索、语义搜索和向量搜索，以适应不同类型的查询和信息需求，确保检索到最相关和上下文丰富的信息。混合检索可以作为检索策略的强大补充，提高 RAG 流程的整体性能。

**嵌入**：

**Pre-Retrieval Process**

* **Fine-tuning Embedding:** 优化嵌入是直接影响RAG效果的重要步骤。精调嵌入的目的是增强检索内容与查询之间的相关性。精调嵌入的作用类似于在生成语音之前调整耳朵，优化检索内容对生成输出的影响。通常，精调嵌入的方法可以分为调整域特定语境下的嵌入和优化检索步骤两类。在处理涉及临时或稀有术语的专业领域中，这些定制的嵌入方法可以提高检索相关性。BGE[1]嵌入模型是一种精调和高性能的嵌入模型，例如由BAAI开发的BGE-large-EN模型[3]。为了为BGE模型进行精调训练，可以使用gpt-3.5-turbo等LLMs根据文档块提出问题，其中问题和答案（文档块）形成精调过程中的训练对。 

* **Dynamic Embedding:** 动态嵌入是根据单词出现的上下文进行调整的，与静态嵌入使用每个单词的单一向量不同。例如，在BERT等Transformer模型中，同一个单词可以根据周围的单词具有不同的嵌入。OpenAI的text-embedding-ada-002模型[4]表明，出现了意外的高余弦相似度结果，尤其是在长度小于5个标记的文本中。理想情况下，嵌入应尽可能包含更多的上下文，以确保"健康"的结果。OpenAI的embeddings-ada-02建立在GPT等大型语言模型的原理基础上，比静态嵌入模型更复杂，能够捕捉一定水平的上下文。虽然在上下文理解方面表现出色，但与GPT-4等最新的全尺寸语言模型相比，它可能没有展现出同样对上下文的敏感性。 

**Post-Retrieval Process**

从数据库中检索到有价值的上下文后，将其与查询合并并输入到LLM中存在挑战。将所有相关文档一次性展示给LLM可能超出上下文窗口限制。将多个文档串联起来形成冗长的检索提示会导致效率低下，引入噪音并阻碍LLM对关键信息的关注。对检索到的内容进行附加处理是必要的，以解决这些问题。

* **ReRank：** 重新排序以将最相关的信息放置在提示的边缘是一个直观的想法。这个概念已经在LlamanIndex、LangChain和HayStack等框架中实现了[15]。例如，Diversity Ranker根据文档多样性重新排序，而LostInTheMiddleRanker则交替将最佳文档放在上下文窗口的开头和结尾。与此同时，为了解决基于向量的模拟搜索在语义相似性上的解释挑战，类似cohereAI rerank [14]、bge-rerank5或LongLLMLingua [16]的方法重新计算相关文本和查询之间的语义相似性。脚注5：https://huggingface.co/BAAI/bge-reranker-large

* **提示压缩：** 研究表明，检索到的文档中的噪音会对RAG的性能产生不利影响。在后处理中，重点是压缩无关上下文，突出关键段落，并减少整体上下文长度。Selective Context [17]和LLMLingua [1]等方法利用小型语言模型计算提示的互信息或困惑度，估计元素的重要性。然而，这些方法在RAG或长上下文场景中可能会丢失关键信息。Recomp [24]通过在不同的粒度上训练压缩器来解决这个问题。Long Context [24]在处理大量上下文时进行分解和压缩，而"Walking in the Memory Maze" [20]则设计了一个分层摘要树来增强LLM对关键信息的感知能力。

# **RAG Pipeline Optimization**
RAG管道优化

对检索过程的优化旨在提高RAG系统的效率和信息质量，目前的研究主要集中在智能地结合各种搜索技术、优化检索步骤、引入认知回溯的概念、灵活应用多样的查询策略以及利用嵌入相似性。这些努力共同致力于在RAG检索中实现效率和上下文信息丰富性之间的平衡。

* **探索混合搜索：**通过智能地结合基于关键词的搜索、语义搜索和向量搜索等各种技术，RAG系统可以利用每种方法的优势。这种方法使RAG系统能够适应不同的查询类型和信息需求，确保一致地检索到最相关和上下文丰富的信息。混合搜索作为检索策略的强大补充，提高了RAG管道的整体性能。
* **递归检索和查询引擎：**优化RAG系统中检索的另一个强大方法是实现递归检索和高级查询引擎。递归检索在初始检索阶段获取较小的文档块以捕捉关键的语义意义。在后续阶段，提供更多上下文信息的较大文档块给语言模型（LM）。这种两步检索方法有助于在效率和上下文丰富响应之间取得平衡。
* **StepBack-prompt：**StepBack-prompt方法[14]集成到RAG过程中，鼓励LLM从特定实例中回溯，并考虑潜在的一般概念或原则。实验结果显示，引入回溯提示显著提高了各种具有推理密集型任务的性能，展示了其对RAG自然适应性的优点。这些增强检索的步骤可以同时应用于回溯提示的答案生成和最终的问答过程。
* **子查询：**可以在不同的场景中采用各种查询策略，包括使用像Llamaldex这样的框架提供的查询引擎，使用树查询，利用向量查询，或者使用最基本的顺序查询块。
* **HyDE：**这种方法基于生成的答案可能在嵌入空间中比直接查询更接近的假设。利用LLM，HyDE对查询生成一个假设的文档（答案），将该文档嵌入，并利用该嵌入来检索与假设文档类似的真实文档。与基于查询的嵌入相似性不同，该方法强调答案与答案之间的嵌入相似性。然而，这种方法在语言模型对讨论的主题不熟悉的情况下可能无法始终产生有利的结果，可能导致错误实例的生成增加。

# **Modular RAG**
模块化的RAG结构摆脱了传统的Naive RAG框架的索引、检索和生成，提供了更大的多样性和灵活性。一方面，它集成了各种方法来扩展功能模块，例如在相似性检索中加入搜索模块，在检索器中应用微调方法[15]。此外，特定的问题导致了重组的RAG模块[24]和迭代方法的出现[25]。模块化的RAG范式成为RAG领域的主流，可以实现串行的流水线或跨多个模块的端到端训练方法。三个RAG范式之间的比较如图3所示。

# **New Modules**
**新模块：**

- **搜索模块（Search Module）：** 与Naive/Advanced RAG中查询与语料库的相似性检索不同，搜索模块针对特定场景，通过使用LLM生成的代码、查询语言（如SQL、Cypher）或其他定制工具，在过程中将直接搜索（附加的）语料库。搜索的数据源可以包括搜索引擎、文本数据、表格数据或知识图谱[26]。

- **记忆模块（Memory Module）：** 利用LLM自身的记忆能力来指导检索，原则是找到与当前输入最相似的记忆。Self-mem[1]使用检索增强的生成模型迭代地创建一个无界的记忆池，将“原始问题”和“双重问题”结合起来。检索增强的生成模型可以利用自己的输出来增强自己，使得生成的文本在推理过程中更接近数据分布，而不是训练数据[26]。

- **额外生成模块（Extra Generation Module）：** 在检索到的内容中，冗余和噪声是常见问题。额外生成模块不直接从数据源中检索，而是利用LLM生成所需的上下文[24]。与直接检索相比，LLM生成的内容更有可能包含相关信息。

- **任务适应模块（Task Adaptable Module）：** 专注于将RAG转化为适应各种下游任务的模块。UPRISE[23]可自动从预先构建的数据池中检索出给定零样本任务输入的提示，增强了任务和模型的通用性。PROMPTAGATOR[1]将LLM用作少样本查询生成器，并基于生成的数据创建特定任务的检索器。利用LLM的泛化能力，PROMPTAGATOR只需少量样本即可创建特定任务的端到端检索器。

- **对齐模块（Alignment Module）：** 查询和文本之间的对齐一直是影响RAG有效性的关键问题。在Modular RAG时代，研究人员发现为检索器添加可训练适配器模块可以有效缓解对齐问题。PRCA[22]利用强化学习训练了一个由LLM奖励驱动的上下文适配器，该适配器位于检索器和生成器之间。在标记的自回归策略中，通过最大化奖励来优化检索到的信息。AAR[21]提出了一种通用插件，通过从已知源LLM中学习LM偏好，来辅助未知或非共同微调的LLM。RRR[15]设计了一个基于强化学习的查询重写模块，以在语料库中将查询与文档对齐。

- **验证模块（Validation Module）：** 在实际应用中，无法保证检索到的信息始终可靠。检索到不相关的数据可能导致LLM产生幻觉。因此，可以在检索文档之后引入额外的验证模块，以评估检索到的文档与查询之间的相关性。这增强了RAG的鲁棒性[21]。

# 3.2.2 New Pattern
3.2.2 新模式

模块化RAG的组织方式具有灵活性，可以根据特定问题背景中进行模块的替换或重构。对于Naive RAG，它由检索和生成这两个模块构成（在一些文献中也称为读取或合成），该框架提供了适应性和丰富性。目前的研究主要探索了两种组织模式，包括添加或替换模块，以及调整模块之间的组织流程。

* **添加或替换模块** 添加或替换模块的策略是保持检索-读取的结构，并引入额外的模块以增强特定的功能。RRR[15]提出了重写-检索-读取的过程，利用强化学习中的LLM性能作为回报来训练重写模块。这允许重写模块调整检索查询，提高阅读器的下游任务性能。类似地，模块可以在类似于生成-读取[21]的方法中进行选择性替换，其中LLM的生成模块替换了检索模块。

图3: RAG的三种模式的比较

Recite-Read [22]将外部检索转为从模型权重中检索，最初使LLM记忆与处理知识密集型自然语言处理任务相关的信息并生成输出。

* **调整模块之间的流程** 在调整模块之间的流程方面，重点是增强语言模型和检索模型之间的交互。DSP[14]引入了演示-搜索-预测的框架，将上下文学习系统视为一个明确的程序，而不是一个终端任务提示，以解决知识密集型任务。ITER-RETGEN [23]利用生成的内容来指导检索，在检索-读取-检索-读取的流程中进行“检索增强生成”和“生成增强检索”的迭代操作。Self-RAG[15]遵循决策-检索-反思-读取的过程，引入了主动判断的模块。这种自适应和多样化的方法允许在模块化RAG框架内动态组织模块。

# 4 Retriever
在RAG的背景下，“R”代表检索，在RAG管道中的作用是从庞大的知识库中检索出前k个相关的文档。然而，设计一个高质量的检索器是一项非常复杂的任务。在本章中，我们将围绕三个关键问题进行讨论：1）如何获取准确的语义表示？2）如何匹配查询和文档的语义空间？3）如何将检索器的输出与大型语言模型的偏好对齐？

# How to acquire accurate semantic representations?
如何获得准确的语义表示？

在RAG中，语义空间是将查询和文档映射到的多维空间。当进行检索时，是在语义空间内进行测量。如果语义表达不准确，对RAG的影响将是致命的。本章将介绍两种方法来帮助我们构建准确的语义空间。

# 4.1.1 Chunk optimization
4.1.1 块优化

在处理外部文档时，第一步是分块(chunking)，以获取细粒度的特征。然后对这些块进行嵌入(embedding)。然而，对于文本块的大小选择过大或过小可能无法取得好的结果。因此，在语料库中找到适合文档的最佳块大小对于确保搜索结果的准确性和相关性至关重要。

在选择分块策略时，重要的考虑因素包括：索引内容的特性、所使用的嵌入模型及其最佳块大小、用户查询的预期长度和复杂度，以及检索结果在特定应用中的使用方式。例如，针对较长或较短的内容，应选择不同的分块模型。此外，不同的嵌入模型在不同块大小下的表现也存在差异；例如，句子嵌入模型(sentence-transformer)更适用于单个句子，而文本嵌入模型(text-embedding-ada-002)对于包含256或512个标记的块更好。此外，用户输入问题文本的长度和复杂度，以及应用程序的特定需求（如语义搜索或问答），都将影响分块策略的选择。这可能直接与所选择的大语言模型的标记限制相关，并可能需要调整块大小。事实上，通过自适应应用多个分块策略可以实现准确的查询结果；没有最好的方式，只有最合适的方式。

当前在RAG中的研究采用了多样的块优化方法来提高检索效率和准确性。诸如滑动窗口技术的技术通过通过多次检索来聚合全局相关信息实现分层检索。Small2big技术在搜索过程中利用小文本块，并提供较大的关联文本块供语言模型处理。摘要嵌入技术对文档摘要进行Top K检索，提供完整的文档上下文。元数据过滤技术利用文档元数据进行过滤。图索引技术将实体和关系转换为节点和连接，在多跳问题的背景下显着提高相关性。这些方法的综合应用已经改进了RAG的检索结果，并提高了性能。

# Fine-tuning Embedding Models
在获取到正确大小的块之后，我们需要通过嵌入模型将块和查询在语义空间中进行嵌入，因此嵌入能否有效地表示语料库至关重要。如今，已经出现了一些优秀的嵌入模型，例如[15]、Voyage[2]、BGE[1]等，它们已经在大规模语料库上进行了预训练，但在应用于特定领域时可能无法准确地表示领域特定的语料库信息。此外，对嵌入模型进行特定任务的微调对于确保模型理解用户查询与内容相关性至关重要，而未经微调的模型可能无法满足特定任务的需求。因此，微调嵌入模型对于下游应用至关重要。嵌入微调方法有两种基本范式。

**领域知识微调** 为了使嵌入模型能够正确理解领域特定的信息，我们需要构建领域特定的数据集来微调嵌入模型。然而，嵌入模型的微调与普通语言模型不同，主要体现在使用的数据集不同。在当前主要的嵌入微调方法中，使用的数据集包括查询、语料库和相关文档三个部分。嵌入模型基于查询在语料库中查找相关文档，然后是否命中查询的相关文档被用作模型的度量指标。

在数据集的构建、模型的微调和评估过程中，每个组件都可能面临许多挑战。在 LlamaIndex [16] 中，已经专门介绍了一系列用于嵌入模型微调过程的关键类和函数，极大地简化了这个过程。通过准备领域知识的语料库并利用其提供的方法，我们可以轻松获得适用于所需领域的专门嵌入模型。

**下游任务的微调** 将嵌入模型适应下游任务同样重要。在使用 RAG 进行下游任务时，一些工作通过利用 LLM 的能力对嵌入模型进行了微调。PROMPTAGATOR[14]将大型语言模型 (LLM) 作为少样本查询生成器，并基于生成的数据创建了特定任务的检索器，以缓解因数据稀缺而难以进行监督微调的问题。LLM-Embedder[15]利用大型语言模型为多个下游任务的数据输出奖励值，通过数据集的硬标签和从LLM得到的软性奖励的两种不同监督信号，对检索器进行微调。

通过领域知识注入和下游任务微调，可以在一定程度上改进语义表示。然而，通过这种方法训练的检索器对大型语言模型来说并不直观有用，因此需要通过LLM的反馈信号直接监督嵌入模型的微调。（关于这一部分将在4.4节中详细介绍）

# How to Match the Semantic Space of Queries and Documents
如何匹配查询和文档的语义空间

在RAG应用中，一些检索器使用相同的嵌入模型对查询和文档进行编码，而其他一些检索器则使用两个模型分别对查询和文档进行编码。此外，用户的原始查询可能存在表达不清晰和缺乏语义信息的问题。因此，将用户查询和文档的语义空间进行对齐是非常必要的。本节介绍了实现这一目标的两种关键技术。

**查询重写**

将查询与文档的语义进行对齐最直观的方法就是重写查询。正如在Query2Doc[23]和ITER-RETGEN[24]中提到的，利用大型语言模型的内在能力，通过引导它生成一个伪文档，然后将原始查询与该伪文档合并形成一个新的查询。在HyDE[1]中，通过使用文本指示器建立查询向量，利用这些指示器生成一个相关的“假设”文档，它可能并不存在，只需要捕捉相关的模式。RRR[13]提出了一个倒序检索和阅读的新框架，专注于查询重写。该方法使用大型语言模型生成查询，然后使用网络搜索引擎检索上下文，最后使用小型语言模型作为训练重写器来为固定的大型语言模型提供服务。STEP-BACKPROMPTING[15]方法可以使大型语言模型进行抽象推理、提取高级概念和原则，并基于此进行检索。最后，在多查询检索方法中，使用大型语言模型生成多个搜索查询，这些查询可以并行执行，检索结果一起输入，对于依赖于多个子问题的单一问题非常有用。

**嵌入转换**

如果有像查询重写这样粗粒度的方法，那么应该有一种更细粒度的嵌入操作的具体实现。在LlamaIndex[13]中，可以在查询编码器之后连接一个适配器，并微调适配器以优化查询嵌入的表示，将其映射到更适合特定任务的潜在空间。当查询和外部文档的数据结构不同，例如，非结构化查询和结构化外部文档时，使查询与文档对齐非常重要。SANTA[13]提出了两种预训练方法，使检索器意识到结构化信息：
1）利用结构化数据和非结构化数据之间的自然对齐关系进行对比学习，进行结构感知预训练。
2）遮蔽实体预测，设计了一种面向实体的遮蔽策略，要求语言模型填充遮蔽的实体。

# How to Aligning Retriever's Output and LLM's Preference
如何调整检索器的输出和LLM的偏好

在RAG流程中，即使我们采用上述技术来提高检索命中率，它可能仍无法改善RAG的最终效果，因为检索到的文档可能不是LLM所需的。因此，本节介绍了两种方法来调整检索器的输出和LLM的偏好。

**LLM监督训练**

许多研究利用大型语言模型的各种反馈信号来微调嵌入模型。AAR [21]通过使用一个编码器-解码器架构的LM为预训练的检索器提供监督信号。通过通过FiD交叉注意力分数确定LM的首选文档，然后使用硬负采样和标准交叉熵损失来微调检索器。最终，经过微调的检索器可以直接用于提升未见目标LLMs的性能，从而在目标任务中表现更好。检索器的训练损失如下：

\[\zeta=\sum_{q}\sum_{d^{+}\in D^{a^{+}}}\sum_{d^{-}\in D^{-}}l\left(f\left(q,d^ {+}\right),f\left(q,d^{-}\right)\right)\] (1)

其中\(D^{a^{+}}\)是LLM在检索集中首选的文档，而\(D^{a^{-}}\)则不是。\(l\)是标准的交叉熵损失。最后，作者指出LLMs可能更倾向于关注易读性较高而不是信息丰富的文档。

REPLU [12]使用检索器和LLM计算检索到的文档的概率分布，然后通过计算KL散度进行监督训练。这种简单而有效的训练方法通过使用LM作为监督信号来提高检索模型的性能，消除了任何特定的交叉注意力机制的需求。检索器的训练损失如下：

\[\zeta=\frac{1}{\left|D\right|}\sum_{x\in D}KL\left(P_{R}\left(d|x\right)||Q_{LM} \left(d|x,y\right)\right)\] (2)

其中\(D\)是一组输入上下文，\(P_{R}\)是检索可能性，\(Q_{LM}\)是每个文档的LM可能性。

UPRISE [3]还使用冻结的大型语言模型来微调Prompt Retriever。但是，语言模型和检索器都将Prompt-Input Pairs作为输入，然后使用大型语言模型给出的分数来监督检索器的训练，相当于使用大型语言模型为数据集标记。Atlas [14]提出了四种细调监督嵌入模型的方法，其中Attention Distillation使用语言模型在输出期间生成的交叉注意力分数进行蒸馏。EMBR2使用期望最大化算法以检索到的文档作为潜在变量进行训练。Perplexity Distillation直接使用模型生成的标记的困惑度进行训练。LOOP引入了一种基于文档删除对LM预测影响的新损失函数，为更好地适应特定任务提供了有效的训练策略。

**插入适配器**

然而，由于诸如利用API实现嵌入功能或本地计算资源不足等因素，微调嵌入模型可能会面临挑战。因此，一些工作选择在外部附加适配器来进行对齐。PRCA [15]通过上下文提取阶段和奖励驱动阶段训练适配器，并基于基于标记的自回归策略优化检索器的输出。TokenFiltering [1]方法计算交叉注意力分数，选择得分最高的输入标记来有效过滤标记。RECOMP [16]提出了抽取式和生成式压缩器，通过选择相关的句子或综合文档信息生成摘要，实现多文档查询焦点摘要。除此之外，一种新颖的方法，PKG [17]通过指令微调将知识注入白盒模型中，并直接替换检索器模块，用以基于查询直接输出相关文档。

# 5 Generator
RAG中的另一个核心组件是生成器，它负责将检索到的信息转化为自然流畅的文本。其设计灵感来自传统的语言模型，但与传统生成模型相比，RAG的生成器通过利用检索到的信息来提高准确性和相关性。在RAG中，生成器的输入不仅包括传统的上下文信息，还包括通过检索器获得的相关文本片段。这使得生成器能够更好地理解问题背后的语境，并产生更加信息丰富的回答。此外，生成器通过检索到的文本来指导，确保生成的内容与检索到的信息之间的一致性。正是由于输入数据的多样性，导致在生成阶段出现了一系列有针对性的努力，旨在更好地适应来自查询和文档的输入数据。我们将通过检索后处理和微调的方面来深入介绍生成器的概念。

# How Can Retrieval Results be Enhanced via Post-retrieval Processing?
如何通过后续处理提高检索结果的质量？

对于未调优的大型语言模型，大多数研究依赖于诸如 GPT-4[2] 等公认的大型语言模型，利用其稳定的内部知识来全面检索文档知识。然而，这些大型模型固有的问题，如上下文长度限制和冗余信息的脆弱性，仍然存在。为了缓解这些问题，一些研究致力于进行后续处理。后续处理是指对检索器从大型文档数据库中检索到的相关信息进行进一步处理、过滤或优化的过程。其主要目的是提高检索结果的质量，从而更好地满足用户需求或进行后续任务。可以将其理解为对检索阶段获取到的文档进行再处理的过程。后续处理的操作通常涉及信息压缩和结果重新排序。

**信息压缩**

尽管检索器可以从广泛的知识库中获取相关信息，但我们仍然面临着处理大量检索文档信息的挑战。一些现有研究试图通过增加大型语言模型的上下文长度来解决这个问题，但目前的大型模型仍然受到上下文限制的限制。因此，在某些情况下，信息压缩是必要的。简而言之，信息压缩的重要性主要体现在以下几个方面：减少噪音，应对上下文长度限制，以及增强生成效果。

PRCA[15]通过训练一个信息提取器来解决这个问题。在上下文提取阶段，给定输入文本\(S_{input}\)，它可以生成一个输出序列\(C_{extracted}\)，该序列表示从输入文档中提取的压缩上下文。训练过程的目标是尽可能减小\(C_{extracted}\)和实际上下文\(C_{truth}\)之间的差异。他们采用的损失函数如下：

\[minL(\theta)=-\frac{1}{N}\sum_{i=1}^{N}C_{truth}^{(i)}log(f.(S_{input}^{(i)}; \theta))\] (3)

其中\(f.\)是信息提取器，\(\theta\)是提取器的参数。RECOMP[16]类似地通过利用对比学习来训练信息压缩器。在这个过程中，对于每个训练数据点，存在一个正样本和五个负样本。编码器在此过程中使用对比损失函数 [17] 进行训练。具体的优化目标如下：

\[-log\frac{e^{sim(x_{i},p_{i})}}{sim(x_{i},p_{i})+\sum_{n_{j}\in N_{i}}e^{sim(x_{i},p_{i})}}\] (4)

其中\(x_{i}\)是训练数据，\(p_{i}\)是正样本，\(n_{j}\)是负样本，sim(x,y)是计算x和y之间的相似度。另外一项研究选择进一步精简文档数量，通过减少检索到的文档数目来提高模型的答案准确性。[14]提出了“Filter-Ranker”范式，将大型语言模型（LLMs）和小型语言模型（SLMs）的优势结合起来。在这个范式中，SLMs作为筛选器，而LLMs作为重排序代理。通过提示LLMs对SLMs识别出的难题样本部分进行重新排序，研究结果表明在各种信息抽取（IE）任务中实现了显著的改进。

# 4.1.1 Rerank
4.1.1 重排序模型的关键作用在于优化从检索器中检索得到的文档集。当向LLMs添加额外的上下文时，LLMs的性能往往会退化，而重排序提供了解决这个问题的有效方法。核心思想是重新排列文档记录，将最相关的项目放置在前面，从而将总文档数量减少到一个固定数量。这不仅解决了在检索过程中可能遇到的上下文窗口扩展的问题，还有助于提高检索效率和响应速度[15]。

在重排序中引入上下文压缩目的是仅基于给定的查询上下文返回相关信息。这种方法的双重意义在于通过减少单个文档的内容和过滤整个文档，将最相关信息集中显示在检索结果中。因此，重排序模型在整个信息检索过程中起到了优化和精炼的作用，为后续LLM处理提供了更有效和准确的输入。

# How to Optimize a Generator to Adapt Input Data?
如何优化生成器以适应输入数据？

在RAG模型中，优化生成器是架构中的一个关键组件。生成器的任务是将检索到的信息转化为相关的文本，从而提供模型的最终输出。优化生成器的目标是确保生成的文本既自然，又有效地利用检索到的文档，以更好地满足用户的查询需求。

在典型的大型语言模型（LLM）生成任务中，输入通常是一个查询。在RAG中，主要的区别在于输入不仅包括查询，还包括检索器检索到的各种文档（结构化/非结构化）。引入其他信息可能会对模型的理解产生重要影响，尤其是对于较小的模型来说如此。在这种情况下，微调模型以适应查询+检索到的文档的输入变得尤为重要。具体而言，在向微调模型提供输入之前，通常会对检索器检索到的文档进行后续处理。需要注意的是，RAG中生成器的微调方法与LLM的一般微调方法本质上是相似的。在这里，我们将简要介绍一些代表性的工作，包括数据（格式化/非格式化）和优化函数。

# 4.1.2 General Optimization Process
4.1.2 总体优化过程

总体优化过程指的是训练数据包含输入和输出的配对，旨在训练模型在给定输入x的情况下生成输出y的能力。在 Self-mem [13] 的工作中，采用了一个相对经典的训练过程。给定输入x，检索相关文档z（在论文中选择Top-1），在整合（x，z）之后，模型生成输出y。该论文利用了两种常见的微调范式，即联合编码器 [1, 14, 15] 和双编码器 [16, 17, 18]。对于联合编码器，使用了基于编码器-解码器的标准模型，其中编码器首先对输入进行编码，解码器通过注意力机制将编码结果组合起来以自回归的方式生成令牌:

\[H=Encoder(x[SEP]m)\] (5)

\[h^{i}=Decoder(CrossAttn(H),y<i)\] (6)

\[P_{G_{\xi}}(.|x,y<i)=Softmax(h^{i})\] (7)

对于双编码器，系统建立了两个独立的编码器，分别负责编码输入（查询、上下文）和文档。然后，解码器按顺序对输出进行双向交叉注意力处理。作者选择使用 Transformer [16] 作为这两种架构的基本组件，并通过优化 \(G_{\xi}\) 的负对数似然（NLL）损失进行优化。

\[H_{x}=SourceEncoder(x)H_{m}=MemoryEncoder(x)\] (8)

\[h^{i}=Decoder(CrossAttn(H_{x},H_{m}),y<i)\] (9)

\[\mathfrak{L}_{nll}=-\sum_{t=1}^{|y|}logP_{G_{\xi}}(y_{t}|x,m,y<t)\] (10)

# 4.1.3 Utilizing Contrastive Learning
4.1.3 利用对比学习

在准备训练数据的阶段，通常会生成输入和输出之间的交互对。在这种情况下，模型只能访问到一个唯一的真实输出，这可能诱导出“曝光偏差”问题[15]：在训练阶段，模型只接触到一个真实反馈，而没有访问任何其他生成的token。这可能会导致模型在应用中性能下降，因为它可能会过度适应训练数据中的特定反馈，而无法有效地泛化到其他场景。因此，SURGE提出了一种图文对比学习方法[16]。对于任何一对输入和输出之间的交互，这种对比学习方法的目标可以定义如下：

\[\mathfrak{L}_{cont}=\frac{1}{2}log\frac{e^{sim(\zeta(z),\xi(h))/\iota}}{\sum_{ h^{\prime}}e^{sim(\zeta(z),\xi(h^{\prime}))/\iota}}+\frac{1}{2}log\frac{e^{ sim(\zeta(z),\xi(h))/\iota}}{\sum_{z^{\prime}}e^{sim(\zeta(z^{\prime}),\xi(h))/ \iota}}\] (11) 这里 \(\zeta\) 和 \(\xi\) 是可学习的线性投影层，\(z\) 是来自编码器的图的平均表示，\(h\) 是解码器表示的平均值，\(z^{\prime}\) 和 \(h^{\prime}\) 分别表示对应的负样本。在给定的文本中，'h' 和 'z' 表示负样本。通过引入对比学习目标，模型可以更好地学习生成多样且合理的回复，而不仅仅是训练数据中出现的一种。这有助于减轻过拟合的风险，并提高模型在实际场景中的泛化能力。

处理涉及结构化数据的检索任务时，SANTA的工作[11]采用了三阶段的训练过程来充分理解结构和语义信息。具体来说，在检索模型的训练阶段，采用了对比学习方法，主要目标是优化查询和文档的嵌入表示。具体的优化目标如下：

\[\mathfrak{L}_{DR}=-log\frac{e^{sim(q,d^{+})}}{e^{f(q,d^{+})}+\sum_{d-\in D-}e ^{sim(q,d^{-})}}\] (12)

其中 q 和 d 是由编码器编码的查询和文档，\(d^{-}\) 和 \(d^{+}\) 分别表示负样本和正样本。在生成模型的初始训练阶段，我们利用对比学习来对齐结构化数据和相应的非结构化数据文档描述。优化目标如上所述。

此外，在生成模型的后期训练阶段，受到参考文献[14, 15]的启发，我们意识到实体语义在学习文本数据表示中的显著有效性。因此，我们首先对结构化数据进行实体识别，然后将实体掩码应用于生成模型训练数据的输入部分，使生成模型能够预测这些掩码。此后的优化目标如下：

\[\mathfrak{L}_{MEP}=\sum_{j=1}^{k}-logP(Y_{d}(t_{j})|X_{d}^{mask},Y_{d}(t_{1},...,j-1))\] (13)

其中 \(Y_{d}(y_{j}\) 表示序列 \(Y_{d}\) 中的第 j 个token。\(Y_{d}=<mask>_{1}\), \(ent_{1}\),..., \(<mask>_{n}\), \(ent_{n}\) 表示包含被掩码实体的地面真值序列。在整个训练过程中，我们通过从上下文中获取必要信息来恢复被掩码的实体，理解文本数据的结构语义，并将结构化的相关实体对齐。我们优化语言模型来填充隐藏的部分，并更好地理解实体语义[21]。

# 6 Augmentation in RAG
本章主要从增强的阶段、增强的数据来源和增强的过程三个维度进行组织，详细阐述了RAG发展中的关键技术。RAG核心组件的分类如图4所示。

# RAG in Augmentation Stages
作为一个知识密集型任务，RAG在语言模型训练的预训练、微调和推理阶段采用不同的技术方法。

# 6.1.1 Pre-training Stage
6.1.1 预训练阶段

自预训练模型出现以来，研究人员通过在预训练阶段采用检索方法来提高开放领域问题回答（QA）的预训练语言模型（PTM）的性能。识别和扩展预训练模型中的隐含知识可能是具有挑战性的。REALM[1]引入了一种更模块化和可解释的知识嵌入方法。按照掩码语言模型（MLM）范例，REALM将预训练和微调都视为检索然后预测的过程，其中语言模型通过基于掩码句子\(x\)预测掩码标记\(y\)来进行预训练，建模\(P(x|y)\)。

RETRO[17]利用检索增强来预训练自回归语言模型，通过从大规模标记数据集中检索并显著减少模型参数，从头开始进行大规模预训练。RETRO与GPT模型共享骨干结构，并引入额外的RETRO编码器来编码从外部知识库中检索的相邻实体的特征。此外，RETRO在其解码器变换器结构中引入了分块交叉注意力层，以有效地整合来自RETRO编码器的检索信息。RETRO的困惑度低于标准GPT模型。此外，它通过在不需要重新训练语言模型的情况下更新检索数据库，提供了更新存储在语言模型中的知识的灵活性[2]。

Atla[16]采用类似的方法，在预训练和微调阶段都使用了基于T5架构[18]的检索机制。在预训练之前，它使用预训练的T5初始化编码器-解码器语言模型主干，使用预训练的Contriever初始化稠密检索器。在预训练过程中，它每1000步刷新一次异步索引。

COG [21]是一个文本生成模型，通过逐渐从现有文本集合中复制文本片段（如单词或短语）来形式化其生成过程。与传统的文本生成模型依次选择单词不同，COG利用高效的向量搜索工具计算文本片段的有意义的上下文表示并对其进行索引。因此，文本生成任务被分解成一系列的复制和粘贴操作，在每个时间步中，不是从独立的词汇表中选择，而是从文本集合中寻找相关的文本片段。COG在各个方面，包括问题回答、领域适应和扩展短语索引，均表现出优异的性能。

另一方面，随着模型参数的快速增加，自回归模型成为主流。研究人员还在探索是否可以使用RAG方法对更大的模型进行预训练。RETRO++[22]是RETRO的扩展，增加了模型的参数规模。研究发现，文本生成质量、事实准确性、低毒性和下游任务准确性均得到持续改进，尤其是在开放领域问题回答等知识密集型任务中。这些研究结果凸显了将预训练的自回归语言模型与检索相结合进行预训练的有前景的方向，作为未来基础模型。

总之，扩展预训练的优点和局限性是显而易见的。从积极的一面来看，这种方法提供了更强大的基础模型，在困惑度、文本生成质量和下游任务性能方面优于标准的GPT模型。此外，与纯预训练模型相比，它利用更少的参数来实现更高的效率。它在处理知识密集型任务方面表现出色，可以通过在特定领域的预料上训练来创建特定领域的模型。然而，缺点包括对大量预训练数据和更大的训练资源的需求，以及更新速度较慢的问题。特别是随着模型规模的增加，增强检索训练的成本相对较高。尽管存在这些局限性，但这种方法在模型的鲁棒性方面具有显著的特点。一旦训练完成，基于纯预训练的增强检索模型无需外部库依赖，增强了生成速度和操作效率。

# Fine-tuning Stage
细化调整阶段

在下游微调阶段，研究人员采用了各种方法来改进信息检索的检索器和生成器，主要应用于开放领域的问答任务。在检索器微调方面，REPIUG[14]将语言模型（LM）视为黑盒，并通过可调的检索模型来增强它。通过通过监督信号获取来自黑盒语言模型的反馈，REPLUG改进了初始的检索模型。UPRISE[15]通过在不同任务集上进行微调，创建了一个轻量级且通用的检索器。这个检索器可以自动为零样本任务提供检索提示，展示了它的普适性和跨任务和模型的改进性能。

同时，微调生成器的方法包括Self-Mem[15]，它通过一个例子的内存池来微调生成器，以及Self-RAG[1]，它通过生成反射令牌来满足主动检索的需求。RADIT[16]方法通过最大化给定检索增强指令下正确答案的概率来微调生成器和检索器。它更新生成器和检索器，以最小化文档和查询之间的语义相似性，从而有效地利用相关背景知识。

此外，SUGRE[14]引入了对比学习的概念。它对检索器和生成器进行端到端的微调，以确保高度详细的文本生成和检索子图。使用基于图神经网络（GNN）的上下文感知子图检索器，SURGE从与正在进行的对话相对应的知识图中提取相关知识。这确保生成的响应忠实地反映了检索到的知识。SURGE为此目的采用了一个稳定而高效的图编码器和一个图文对比学习目标。

总而言之，在微调阶段的增强方法具有以下几个特点。首先，微调LLM和检索器使其能够更好地适应特定任务，并提供了微调一个或两者同时进行的灵活性，如RePlug[13]和RA-DIT[11]所示。其次，这种微调的好处扩展到适应不同的下游任务，如UPRISE[15]所示，使模型更具通用性。此外，微调使模型能够更好地适应不同的数据结构和各种语料库，对于以图结构为特点的语料库尤为有利，正如SUGRE方法所强调的。

然而，在此阶段的微调中存在一些限制，例如需要专门为RAG微调准备的数据集，以及与推理阶段相比需要大量的计算资源。总体而言，在微调阶段，研究人员可以根据特定需求和数据格式对模型进行定制，降低资源消耗，同时保留调整模型输出样式的能力。

# Inference Stage
在推理阶段中，将RAG方法与LLM集成已成为一种普遍的研究方向。值得注意的是，Naive RAG的研究范式依赖于在推理阶段期间将检索内容整合到模型中。

为了克服Naive RAG的局限性，研究人员在推理阶段中引入了更丰富的上下文。DSP框架依赖于一个复杂的流水线，其中涉及冻结的语言模型（LM）和检索模型（RM）之间的自然语言文本传递，为模型提供更丰富的上下文以提高生成质量。PKG则为LLMs配备了一个知识引导模块，允许访问相关知识而不改变LLMs的参数，使模型能够执行更复杂的任务。此外，CREA-ICL利用同步检索跨语言知识以辅助获取额外信息，而RECITE通过从LLMs中抽样一个或多个段落来形成上下文。

在推理阶段，优化RAG的过程可以有益于适应更具挑战性的任务。例如，ITRG通过迭代地检索和搜索正确的推理路径来增强需要多步推理的任务的适应性。ITER-RETGEN采用迭代方法将检索和生成结合起来，实现“检索增强生成”和“生成增强检索”的交替过程。

另一方面，IRCOT将RAG和CoT的概念合并，采用交替的CoT引导检索，并利用检索结果来改善CoT。这种方法显著提高了GPT-3在各种问答任务中的性能，突显了整合检索和生成的潜在优势。

总之，推理阶段的增强方法具有轻量级、成本效益高、无需额外训练和利用强大预训练模型的优势。其主要优势在于在微调过程中冻结LLMs的参数，着重提供更适合需求的上下文，具有快速和低成本的特点。然而，这种方法也存在一些局限性，包括需要额外的数据处理和过程优化，同时受到基础模型能力的限制。通常，该方法常常与过程优化技术（如逐步推理、迭代推理和自适应检索）相结合，以更好地满足不同任务的需求。

# Augmentation Data Source
数据源对于RAG的有效性至关重要。不同的数据源提供了不同细节和知识维度，需要采用不同的处理方法。主要有三类数据源：非结构化数据、结构化数据和由LLM生成的内容。

# Augmented with Unstructured Data
增强的非结构化数据

非结构化数据主要包括文本数据，通常来源于纯文本语料库。此外，其他文本数据也可以作为检索源，例如用于大模型微调的提示数据[15]和跨语言数据[17]。

就文本粒度而言，除了常见的文本块（包括句子）外，检索单位可以是标记（例如kNN-LM[17]），短语（例如NPM[15]，COG[21]）和文档段落。更细粒度的检索单位通常能更好地处理罕见模式和领域外场景，但会增加检索成本。

在单词级别上，FLARE采用主动检索策略，只在语言模型生成低概率词语时进行检索。该方法涉及生成一个临时的下一个句子，用于检索相关文档，然后在检索到的文档条件下重新生成下一个句子，以预测后续句子。

在块级别上，RETRO利用前一个块来检索最近的邻近块，并将该信息与前一个块的上下文信息结合起来，指导下一个块的生成。RETRO通过从检索数据库中检索最近的邻近块\(N(C_{i-1})\)，然后通过交叉注意力将之前的块\(C_{1},\ldots,C_{i-1}\)的上下文信息和\(N(C_{i-1})\)的检索信息融合起来，以指导下一个块\(C_{i}\)的生成。为了保持因果关系，第\(i\)个块\(C_{i}\)的自回归生成只能使用前一个块\(N(C_{i-1})\)的最近邻，而不能使用\(N(C_{i})\)。

结构化数据增强

像知识图谱（KG）这样的结构化数据源逐渐融入到RAG范式中。验证过的KG可以提供更高质量的上下文，降低模型产生幻觉的可能性。

RET-LLM[14]通过从过去的对话中提取关系三元组构建个性化的知识图谱存储，以供将来使用。SUGRE[17]使用图神经网络（GNN）嵌入从知识图谱中检索到的相关子图，以防止模型生成上下文不相关的回复。SUGRE[17]采用了一种将图结构反映到PTMs表示空间中的图编码方法，并利用图文模式之间的多模态对比学习目标，以确保检索到的事实与生成的文本一致。KnowledgeGPT[18]以代码形式生成知识库（KB）的搜索查询，并包含预定义的KB操作函数。除了检索，KnowledgeGPT还具备将知识存储在个性化知识库中以满足个体用户需求的能力。这些结构化数据源为RAG提供了更丰富的知识和上下文，从而提高了模型的性能。

由LLM生成的内容增强RAG

观察到由RAG召回的辅助信息并不总是有效的，甚至可能产生负面影响，一些研究深入挖掘LLM内部知识来扩展RAG的范式。这种方法利用LLM自身生成的内容进行检索，旨在提高下游任务的性能。以下是此类别中一些值得注意的研究：

SKR[18]使用带标签的训练集，将模型可以直接回答的问题分类为已知问题，将需要检索增强的问题分类为未知问题。模型被训练用于辨别问题是否为已知问题，只对被标识为未知问题的输入应用检索增强，然后直接回答其余问题。

GenRead[19]将LLM生成器用作检索器。实验结果表明，LLM生成的上下文文档中包含正确答案的情况比通过Naive RAG检索得到的情况更常见。生成的答案也表现出更高的质量。作者将这归因于生成文档级上下文的任务与因果语言建模的预训练目标之间的对齐，使得模型参数中存储的世界知识能够更好地利用起来。

Selfmen[10]迭代使用检索增强的生成器创建一个无限的记忆池。通过使用记忆选择器，选择一个输出作为后续生成的记忆。这个输出作为原始问题的对偶问题。通过结合原始问题和对偶问题，检索增强的生成模型可以利用自身的输出来提升自身性能。

这些多样化的方法展示了在RAG检索增强中的创新策略，旨在提升模型的性能和效果。

# Augmentation Process
**增强检索过程**

通常，RAG的研究只进行单次的检索和生成过程。然而，单次检索可能包含冗余信息，导致“中间迷失”现象[12]。这些冗余信息可能会掩盖关键信息，或者包含与真实答案相反的信息，对生成效果产生负面影响[11]。此外，单次检索获取的信息在需要多步推理的问题上受到限制。

目前，优化检索过程的方法主要包括迭代检索和自适应检索。这些方法允许模型在检索过程中进行多次迭代或自适应调整来更好地适应不同的任务和场景。

**迭代检索**

根据原始查询和生成的文本定期收集文档可以为LLMs提供额外的材料[1]（Arora _et al._, 2023）。多次迭代检索中提供额外的参考资料已经改善了后续答案生成的鲁棒性。然而，这种方法可能存在语义不连续的问题，可能导致收集到噪声和无用信息，因为它主要依赖于一系列n个标记来分隔生成的和检索到的文档。

递归检索和多跳检索用于特定的数据场景。递归检索可以通过结构化索引首先处理数据，然后逐层检索。在检索层次丰富的文档时，可以为整个文档或长PDF的每个部分进行摘要。然后，基于摘要进行检索。在确定文档后，可对内部块执行第二次检索，实现递归检索。多跳检索通常用于进一步挖掘图结构化数据源中的信息[10]。

一些方法迭代检索和生成的步骤。ITER-RETGEN [22] 共同使用“检索增强生成”和“生成增强检索”来完成需要复制信息的任务。也就是说，模型使用完成任务所需的内容来回应输入任务，而这些目标内容作为检索更相关知识的信息上下文。这有助于在另一个迭代中生成更好的响应。

IRCoT [13] 还探索了为每个生成的句子检索文档的方法，在思考链的每一步引入了检索。它使用了CoT来指导检索，并使用检索结果来改进CoT，确保语义完整性。

**自适应检索**

之前的两节描述的RAG方法都采用了一种被动的检索优先的方法。这种方法包括查询相关文档并基于上下文输入到LLM中，可能会导致效率问题。自适应检索方法，如Flare [16] 和Self-RAG [17] 引入了优化RAG检索过程，使LLM能够主动判断检索的时机和内容。这有助于提高检索到的信息的效率和相关度。

事实上，LLM主动使用工具并做出判断的方式并不是源自RAG，而是在大型模型的代理中广泛应用[21, 22, 23]。Graph-Toolformer [23] 的检索步骤大致分为：LLMs主动使用检索器，Self-Ask 和DSP [14] 尝试使用少样本提示来触发LLM的搜索查询。当LLM认为有必要时，它可以决定搜索相关的查询来收集所需材料，类似于代理的工具调用。

WebGPT [15] 使用强化学习框架自动训练GPT-3模型使用搜索引擎进行文本生成。它使用特殊令牌执行操作，包括在搜索引擎上查询、滚动排名和引用参考文献。这使得GPT-3能够利用搜索引擎进行文本生成。

另一方面，Flare [16] 自动确定检索的时机，并根据生成文本的概率处理定期文档检索的成本。它使用概率作为LLM在生成过程中的置信度指标。当一个术语的概率降至预定义阈值以下时，信息检索系统将检索参考文献并移除概率较低的术语。此方法旨在处理LLM可能需要额外知识的情况。

Self-RAG [17] 引入了一个重要创新，称为反思标记。这些特殊标记用于回顾输出，并分为检索和评论两种类型。模型可以自主决定何时检索段落，或使用设置的阈值触发检索。当需要检索时，生成器同时处理多个段落，并进行片段级的束搜索，以获得最佳序列。每个子分区的分数使用评论分数进行更新，这些权重可以在推理过程中进行调整，以定制模型的行为。Self-RAG框架还允许LLM自主确定是否需要回忆，避免训练额外的分类器或依赖于NLI模型。这增强了模型对输入的自主判断能力，生成准确的答案。

# 7 RAG Evaluation
在探索RAG（检索增强生成）的发展和优化过程中，有效评估其性能已成为一个核心问题。本章主要讨论评估的方法、RAG的关键指标、它应具备的能力以及一些主流的评估框架。

# Evaluation Methods
评估RAG模型的有效性主要有两种方法：独立评估和端到端评估[18]。

# Independent Evaluation
Independent Evaluation（独立评估）部分的翻译如下：

独立评估包括对检索模块和生成模块进行评估。

1. **检索模块** 通常使用一套衡量系统（如搜索引擎、推荐系统或信息检索系统）在根据查询或任务对项目进行排序时的效果的度量指标来评估 RAG 的检索模块的性能。例如，命中率（Hit Rate）、MRR、NDCG、准确率等。
2. **生成模块** 这里的生成模块指的是通过将检索到的文档补充到查询中形成的增强或综合输入，与最终的答案/回答生成（通常进行端到端评估）有所不同。对生成模块的评估指标主要集中在上下文相关性上，衡量检索到的文档与查询问题的相关性。

# End-to-End Evaluation
端到端评估（End-to-end evaluation）评估RAG模型生成的最终响应对于给定输入的相关性和对齐性。从内容生成目标的角度来看，评估可以分为无标签内容和带标签内容。无标签内容评估指标包括答案忠实度、答案相关性、无害性等；而带标签内容评估指标包括准确度和EM（精确匹配）。此外，从评估方法的角度来看，端到端评估可以分为手动评估和使用LLM进行自动评估。上述总结了RAG端到端评估的一般情况。此外，基于RAG在特定领域的应用，还采用了特定的评估指标，例如问题回答任务中的EM[19]，摘要任务中的UniEval和E-F1[16]，以及机器翻译中的BLEU[23]。这些指标有助于了解RAG在不同特定应用场景中的性能。

# Key Metrics and Abilities
现有的研究往往缺乏对检索增强生成模型（Retrieval-Augmented Generation，RAG）对不同LLM的影响进行严谨评估。在大多数情况下，对RAG在各种下游任务中的应用以及不同的检索器可能会产生不同的结果。然而，一些学术和工程实践关注了对RAG的一般评估指标以及评估其性能所需的能力。本节主要介绍用于评估RAG效果的关键指标和评估其性能所需的基本能力。

# Key Metrics
最近OpenAI的报告[18]提到了优化大型语言模型（LLMs）的各种技术，包括RAG及其评估指标。此外，最新的评估框架如RAGAS[11]和ARES[22]也涉及RAG的评估指标。总结这些工作，重点关注以下三个核心指标：回答的准确性（Faithfulness of the answer）、回答的相关性（Answer Relevance）和上下文的相关性（Context Relevance）。

1. **回答的准确性**（Faithfulness）：这个指标强调模型生成的答案必须与给定的上下文一致，确保答案与上下文信息相符，不偏离或矛盾。这方面的评估对于解决大模型中的幻觉至关重要。

2. **回答的相关性**（Answer Relevance）：这个指标强调生成的答案需要与提出的问题直接相关。

3. **上下文的相关性**（Context Relevance）：这个指标要求检索到的上下文信息尽可能准确和精准，避免无关内容。毕竟，处理长文本对LLMs而言是昂贵的，过多的无关信息会降低LLMs在利用上下文时的效率。OpenAI的报告还提到了“上下文召回率”作为一个补充指标，用来衡量模型检索到回答问题所需的所有相关信息的能力。这个指标反映了RAG检索模块的搜索优化水平。低召回率意味着有可能需要优化搜索功能，比如引入重新排序机制或微调嵌入，以确保检索到更相关的内容。

# Key abilities
RGB[10]的研究分析了不同大型语言模型在RAG方面的四种基本能力，包括噪声鲁棒性、负面拒绝、信息整合和对抗性稳健性，为检索增强生成建立了基准。RGB关注以下四种能力：

1. **噪声鲁棒性**：该能力评估模型在处理与问题相关但不包含有用信息的噪声文档时的效率。
2. **负面拒绝**：当模型检索到的文档缺乏回答问题所需的知识时，模型应正确地拒绝回答。在负面拒绝的测试环境中，外部文档仅包含噪声。理想情况下，大语言模型应发出“缺乏信息”或类似的拒绝信号。
3. **信息整合**：该能力评估模型是否可以整合来自多个文档的信息以回答更复杂的问题。
4. **对抗性稳健性**：此测试旨在评估模型在接收到有关检索信息的潜在风险的指示时，是否可以识别和处理文档中已知的错误信息。对抗性稳健性测试包括模型可以直接回答的问题，但相关的外部文档包含事实错误的情况。

# Evaluation Frameworks
最近，LLM社区一直在探索使用“LLMs作为评判者”进行自动评估，在这方面，许多人利用强大的LLMs（如GPT-4）来评估它们自己的LLM应用输出。Databricks的实践使用GPT-3.5和GPT-4作为LLM评判员来评估他们的聊天机器人应用，表明使用LLMs作为自动评估工具是有效的[10]。他们认为这种方法也可以在成本效益上高效地评估基于RAG的应用。

在RAG评估框架领域，RAGAS和ARES相对较新。这些评估的核心关注点是三个主要指标：答案的准确性、答案的相关性和上下文的相关性。此外，行业提出的开源库TruLens也提供了类似的评估模式。这些框架都使用LLMs作为评判者进行评估。由于TruLens与RAGAS相似，本章将重点介绍RAGAS和ARES。

# RAGAS
**算法原理**

1. 评估回答的真实性：使用LLM将回答分解为单独的陈述，并验证每个陈述是否与上下文一致。最终，通过比较支持的陈述数量与总陈述数量，计算出“真实性得分”。
2. 评估回答的相关性：使用LLM生成潜在的问题，并计算这些问题与原始问题之间的相似度。回答相关性得分通过计算所有生成问题与原始问题的平均相似度来得出。
3. 评估上下文的相关性：使用LLM直接提取与问题直接相关的句子，并将这些句子与上下文中的总句子数的比例作为上下文相关性得分。

ARES旨在自动评估RAG系统在上下文相关性、回答真实性和回答相关性三个方面的性能。这些评估指标与RAGAS类似。然而，作为一种较新的基于简单手写提示的评估框架，RAGAS在适应新的RAG评估设置方面有限，这也是ARES工作的重要意义之一。此外，正如其评估结果所示，ARES的性能显著低于RAGAS。

ARES通过使用少量手动注释数据和合成数据来降低评估成本，并利用预测驱动推理（PDR）提供统计置信区间，提高评估的准确性。

**算法原理**

1. 生成合成数据集：ARES首先使用语言模型从目标语料库的文档中生成合成问题和回答，以创建正面和负面样本。
2. 准备LLM评估模型：然后，ARES使用合成数据集微调轻量级语言模型，使其能够评估上下文相关性、回答真实性和回答相关性。
3. 使用置信区间对RAG系统进行排名：最后，ARES应用这些评估模型对RAG系统进行打分，并结合手动注释的验证集使用PPI方法生成置信区间，可靠地评估RAG系统的性能。

# 8 Future Prospects
在本章中，我们将探讨RAG的三个未来展望，分别是垂直优化、水平扩展和RAG的生态系统。

# Vertical Optimization of RAG
尽管在过去一年中，RAG技术取得了快速进展，但其垂直领域仍需要进一步研究。

首先，RAG中的长上下文问题是一个重要挑战。正如文献[15]中提到的，RAG的生成阶段受到LLMs上下文窗口的限制。如果窗口过短，可能包含的相关信息不足；如果窗口过长，可能导致信息丢失。目前，在LLM发展中，扩展上下文窗口，甚至达到无限上下文的程度，是一个关键方向。然而，一旦上下文窗口的限制被解除，RAG应该如何适应仍然是一个值得关注的问题。

其次，提高RAG的鲁棒性是另一个重要的研究重点。如果检索过程中出现无关噪声，或者检索到的内容与事实相矛盾，都会严重影响RAG的有效性。这种情况被形象地称为“打开一本书却看到了有毒的蘑菇”。因此，提高RAG的鲁棒性越来越受到研究者的关注，如文献[16, 17]所示。

第三，RAG和Fine-tuning的协同作用也是一个主要的研究重点。混合方法逐渐成为RAG的主流方法，例如RAGIT [10]。如何协调两者之间的关系，同时获得参数化和非参数化的优势，是一个需要解决的问题。

最后，RAG的工程实践是一个重要的研究领域。易于实现和与企业工程需求的对齐，推动了RAG的迅速发展。然而，在工程实践中，如何提高大规模知识库场景下的检索效率和文档召回率，以及如何确保企业数据的安全性，如防止LLMs泄露文档的来源、元数据或其他信息，都是需要解决的重要问题[1]。

# Horizontal expansion of RAG
RAG的水平拓展

RAG的研究在水平领域迅速扩展。从最初的文本问答领域开始，RAG的思想逐渐应用到更多的模态数据，如图像、代码、结构化知识、音频和视频等。在这方面已经有许多相关工作。

在图像领域，BLIP-2[1]提出了使用冻结图像编码器和大规模语言模型进行视觉语言预训练的方法，降低了模型训练的成本。此外，该模型可以从零样本生成图像到文本的转换。在文本生成领域，VBR[18]方法用于生成图像，以引导语言模型的文本生成，对于开放式文本生成任务具有显著效果。

在代码领域，RBPS[19]用于与代码相关的小规模学习。通过编码或频率分析，自动检索与开发者任务相似的代码示例。这种技术在测试断言生成和程序修复任务中证明了其有效性。在结构化知识领域，类似CoK[1]的方法首先从知识图谱中获取与输入问题相关的事实，然后以提示的形式将这些事实添加到输入中。这种方法在知识图谱问答任务中表现良好。

对于音频和视频领域，GSS[17]方法从口头词汇库中检索并连接音频片段，将机器翻译数据立即转换为语音翻译数据。UEOP[10]通过引入外部离线策略实现了端到端自动语音识别的新突破，通过基于KNN的注意力融合来偏置ASR，从而有效缩短领域适应时间。Vid2Seq[17]架构通过引入特殊的时间标记，增强了语言模型，使其能够在同一输出序列中无缝预测事件边界和文本描述。

# Ecosystem of RAG
【RAG的生态系统】

RAG的生态系统是指在实际应用中构建和发展的一系列支持RAG模型的技术、工具和资源。这个生态系统的发展对于进一步推动RAG模型的应用和研究至关重要。

首先，其中一个关键组成部分是外部知识库。外部知识库包含了大量的结构化和非结构化的知识，可以为RAG模型提供丰富的背景信息和参考知识。这些知识库可以是已有的在线百科全书、在线论坛、机构数据库等。

其次，为了更有效地检索和利用外部知识库，研究人员和工程师们需要开发高效的检索模型和算法。这些检索模型和算法能够快速准确地从外部知识库中检索到相关信息，并将其传递给RAG模型进行生成。

此外，由于RAG模型需要大量的计算资源和存储资源，搭建和优化相应的硬件和软件环境也是非常重要的。高性能计算平台、分布式存储系统以及优化的机器学习框架都可以为RAG模型的性能提供支持。

另外，为了更好地支持RAG模型的研究和应用，建立一个活跃的社区也是至关重要的。这个社区可以促进研究人员之间的合作与交流，共同解决问题，并分享最新的研究成果和技术进展。

总之，RAG的生态系统是一个涵盖了外部知识库、检索模型、计算资源和社区支持等多个方面的综合体系。通过不断发展和完善这个生态系统，可以进一步推动RAG模型的应用和发展，为解决现实问题提供更加准确和可靠的生成能力。

# 8.2.1 Downstream Tasks and Evaluation
8.2.1 下游任务和评估

通过整合来自广泛知识库的相关信息，RAG 在增强语言模型处理复杂查询和生成信息丰富响应方面表现出显著潜力。众多研究表明，RAG 在各种下游任务中表现出色，例如开放式问答和事实验证。RAG 模型不仅改善了下游应用中信息的准确性和相关性，还增加了响应的多样性和深度。

鉴于 RAG 的成功，探索该模型在多领域应用中的适应性和普适性将成为未来的研究方向。这包括在医学、法律和教育等专业领域知识问答中的应用。在专业领域知识问答等下游任务的应用中，RAG 可能比微调具有更低的训练成本和更好的性能优势。

同时，改进 RAG 的评估体系，以评估和优化其在不同下游任务中的应用，对于模型的效率和特定任务的效益至关重要。这包括为不同下游任务开发更准确的评估指标和框架，如上下文相关性、内容创意和无害性等。

此外，通过 RAG 提升模型的可解释性，使用户能够更好地理解模型是如何以及为何作出特定的响应，也是一项有意义的任务。

# 8.2.2 Technical Stack
8.2.2 技术栈

在RAG的生态系统中，相关技术栈的发展起到了推动作用。例如，随着ChatGPT的普及，LangChain和LLamaIndex迅速变得广为人知。它们提供了丰富的与RAG相关的API，并逐渐成为大模型时代不可或缺的技术之一。与此同时，新型技术栈不断被开发出来。虽然它们的功能不如LangChain和LLamaIndex多，但它们更注重自身的独特特点。例如，Flowise AI6强调低代码，使用户可以通过拖放而不是编写代码来实现以RAG为代表的各种AI应用。其他新兴技术包括HayStack、Meltno和Cohere Coral。

除了AI本地框架外，传统软件或云服务提供商也扩展了其服务范围。例如，由向量数据库公司Weaviate提供的Verba7专注于个人助手。亚马逊基于RAG思想为用户提供了智能企业搜索服务工具Kendra。用户可以通过内置连接器在不同的内容库中进行搜索。

技术栈的发展与RAG相互促进。新技术对现有技术栈提出了更高的要求，而技术栈功能的优化进一步推动了RAG技术的发展。总体而言，RAG的技术工具链已初步形成，并且许多企业级应用已逐渐出现，但一个全面的一体化平台仍然需要完善。

# 9 Conclusion
本文全面探讨了检索增强型生成（RAG）技术，该技术利用外部知识库补充大型语言模型（LLM）的上下文并生成响应。值得注意的是，RAG将LLM的参数化知识与非参数化的外部知识相结合，缓解了幻觉问题，通过检索技术识别及时信息，并提高了响应准确性。此外，通过引用来源，RAG增加了模型输出的透明度，增强了用户对模型输出的信任。RAG还可以根据特定领域进行定制，通过对相关文本语料进行索引。RAG的发展和特点总结为三个范式：Naive RAG、Advanced RAG和Modular RAG，每个范式都有其模型、方法和不足之处。Naive RAG主要涉及“检索-阅读”过程。Advanced RAG使用更精细的数据处理，优化了知识库索引，引入了多个或迭代的检索。随着探索深入，RAG将其他技术（如微调）整合在一起，形成了模块化RAG范式，丰富了RAG过程并提供了更大的灵活性。

在接下来的章节中，我们进一步详细分析了RAG的三个关键部分。第4章介绍了RAG的检索器，如何处理语料库以获得更好的语义表示，如何缓解查询和文档之间的语义差距，以及如何调整检索器以适应生成器。第5章解释了生成器如何通过后处理检索文档获得更好的生成结果，避免“中间丢失”问题，以及调整生成器以适应检索器的方法。随后，在第6章中，我们从检索阶段、检索数据源和检索过程的角度，回顾了当前的检索增强方法。

第7章解释了如何评估当前的RAG方法，包括评估方法、关键指标和当前评估框架。最后，我们对RAG的潜在未来研究方向进行了展望。作为一种结合检索和生成的方法，RAG在未来的研究中有许多潜在的发展方向。通过不断改进技术和扩大应用领域，可以进一步提高RAG的性能和实用性。

